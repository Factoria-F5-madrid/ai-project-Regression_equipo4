{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060887a1",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-19T10:16:57.267099Z",
     "iopub.status.busy": "2025-09-19T10:16:57.266751Z",
     "iopub.status.idle": "2025-09-19T10:16:59.458045Z",
     "shell.execute_reply": "2025-09-19T10:16:59.456692Z"
    },
    "papermill": {
     "duration": 2.196604,
     "end_time": "2025-09-19T10:16:59.459949",
     "exception": false,
     "start_time": "2025-09-19T10:16:57.263345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13545db1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-19T10:16:59.465742Z",
     "iopub.status.busy": "2025-09-19T10:16:59.465230Z",
     "iopub.status.idle": "2025-09-19T10:17:00.386423Z",
     "shell.execute_reply": "2025-09-19T10:17:00.385046Z"
    },
    "papermill": {
     "duration": 0.925914,
     "end_time": "2025-09-19T10:17:00.388128",
     "exception": false,
     "start_time": "2025-09-19T10:16:59.462214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../datasets/train.csv')\n",
    "'''Removing the 'id' column is a common preprocessing step in machine learning workflows.\n",
    "id's do not carry predictive information and could interfere with model training if left in the dataset.\n",
    " By dropping this column from both training and test sets, you ensure that only relevant features are used for modeling'''\n",
    "train_df.drop('id',inplace=True,axis=1)\n",
    "print('Columns:', list(train_df.columns))\n",
    "print('Info:')\n",
    "train_df.info()\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3372cb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check pseudonull values and uniqueness faults\n",
    "print(\"\\nUnique values in each column (nan is not a value):\")\n",
    "for col in train_df.columns:\n",
    "    print(f\"{col}: {train_df[col].nunique()} unique values\")\n",
    "    if col != \"id\":\n",
    "        try:\n",
    "            unique_vals = train_df[col].unique()\n",
    "            # Convert to string to handle mixed types, then sort\n",
    "            unique_vals_str = [str(val) for val in unique_vals]\n",
    "            print(f\"  Values: {sorted(unique_vals_str)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Values: {list(train_df[col].unique())}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fee5a45",
   "metadata": {},
   "source": [
    "\"brand\" column has no pseudonull values and unique values are OK\n",
    "\"model\" column has no pseudonull values and unique values are OK\n",
    "\"model_year\" column has no pseudonull values and unique values are OK\n",
    "\"mileage\" column has no pseudonull values and unique values are OK\n",
    "\"fuel_type\" column has pseudonull values \"-\"\n",
    "\"engine\" column has pseudonull values \"-\"\n",
    "\"transmission\" column has pseudonull values \"-\"\n",
    "\"ext_col\" column has pseudonull values \"-\"\n",
    "\"int_col\" column has pseudonull values \"-\"\n",
    "\"accident\" column has no pseudonull values and unique values are OK\n",
    "\"clean_title\" column has no pseudonull values and unique values are OK\n",
    "\"price\" column has no pseudonull values and unique values are OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb88fe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace \"-\" values by nan\n",
    "df_clean = train_df.copy()\n",
    "missing_values = ['-', '—', '–', '−']\n",
    "df_clean = df_clean.replace(missing_values, np.nan)\n",
    "print('Info after replacing \"-\" by nan:')\n",
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda75ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(df_clean, figsize=(12, 8))\n",
    "plt.title('Missing Values distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb4c66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for duplicate rows\n",
    "duplicates = df_clean[df_clean.duplicated(keep=False)]#df_clean.duplicated(keep=False) returns a boolean Series indicating if a row is duplicated in the  DataFrame\n",
    "duplicates.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851c260c",
   "metadata": {},
   "source": [
    "There are no duplicate rows in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0c59bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete any row with nan value\n",
    "df_clean.dropna(inplace=True)\n",
    "print('Info after deleting nan rows')\n",
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01f1b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def feature_engineering(df_clean):\n",
    "    current_year = datetime.now().year\n",
    "\n",
    "    df_clean['age'] = current_year - df_clean['model_year']\n",
    "    df_clean['milage_per_year'] = df_clean['milage']/df_clean['age']\n",
    "\n",
    "    def extract_horsepower(engine):\n",
    "        try:\n",
    "            return float(engine.split('HP')[0])\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def extract_engine_size(engine):\n",
    "        try:\n",
    "            return float(engine.split(' ')[1].replace('L', ''))\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    df_clean['horsepower'] = df_clean['engine'].apply(extract_horsepower)# when splitting fails nan values are assigned\n",
    "    df_clean['engine_size'] = df_clean['engine'].apply(extract_engine_size)\n",
    "    df_clean['power_to_weight_ratio'] = df_clean['horsepower']/df_clean['engine_size']\n",
    "\n",
    "    luxury_brands =  ['Mercedes-Benz', 'BMW', 'Audi', 'Porsche', 'Land', \n",
    "                    'Lexus', 'Jaguar', 'Bentley', 'Maserati', 'Lamborghini', \n",
    "                    'Rolls-Royce', 'Ferrari', 'McLaren', 'Aston', 'Maybach']\n",
    "    df_clean['Is_Luxury_Brand'] = df_clean['brand'].apply(lambda x: 1 if x in luxury_brands else 0)\n",
    "\n",
    "    df_clean['Accident_Impact'] = df_clean.apply(lambda x: 1 if x['accident'] == 1 and x['clean_title'] == 0 else 0, axis=1)\n",
    "    # Nan values are generated throug feature enginereeing. Drop rows with any NaN values after feature extraction\n",
    "    df_clean = df_clean.dropna()\n",
    "    return df_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3f6668",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = feature_engineering(df_clean)\n",
    "print(\"\\nNaNs in df_clean after feature engineering:\", df_clean.isnull().sum())\n",
    "df_clean.to_csv('../datasets/train_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2686c1c",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "Now that we have cleaned our dataset, let´s get its metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6176eb9",
   "metadata": {},
   "source": [
    "## Univariate Analysis\n",
    "\n",
    "Now we'll perform a comprehensive univariate analysis to understand the distribution and characteristics of each variable in our cleaned dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eada93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNIVARIATE VISUALIZATIONS\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"UNIVARIATE ANALYSIS - VISUALIZATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Set up the plotting style\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (15, 30)  # taller for 12 plots\n",
    "\n",
    "# 1. NUMERIC VARIABLES PLOTS\n",
    "numeric_columns = df_clean.select_dtypes(include=['number']).columns\n",
    "n_cols = 3\n",
    "n_rows = 6\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 30))\n",
    "fig.suptitle('Univariate Analysis - Numeric Variables', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, col in enumerate(numeric_columns):\n",
    "    row = idx // n_cols\n",
    "    col_idx = idx % n_cols\n",
    "    # Histogram in the upper half of each cell\n",
    "    if col == 'Accident_Impact':\n",
    "        axes[row, col_idx].hist(df_clean[col].dropna(), bins=[-0.5,0.5,1.5], alpha=0.7, color='skyblue', edgecolor='black', rwidth=0.8)\n",
    "        axes[row, col_idx].set_xlim(-0.1, 1.1)\n",
    "        axes[row, col_idx].set_xticks([0, 1])\n",
    "    else:\n",
    "        axes[row, col_idx].hist(df_clean[col].dropna(), bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[row, col_idx].set_title(f'{col} - Distribution', fontweight='bold')\n",
    "    axes[row, col_idx].set_xlabel(col)\n",
    "    axes[row, col_idx].set_ylabel('Frequency')\n",
    "    axes[row, col_idx].grid(True, alpha=0.3)\n",
    "\n",
    "# Hide any unused subplots (remove empty histograms)\n",
    "for idx in range(len(numeric_columns), n_rows * n_cols):\n",
    "    row = idx // n_cols\n",
    "    col_idx = idx % n_cols\n",
    "    fig.delaxes(axes[row, col_idx])\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.5, 1, 0.97])\n",
    "plt.show()\n",
    "\n",
    "# Boxplots for numeric columns\n",
    "fig2, axes2 = plt.subplots(n_rows, n_cols, figsize=(18, 30))\n",
    "fig2.suptitle('Univariate Analysis - Numeric Variables (Boxplots)', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, col in enumerate(numeric_columns):\n",
    "    row = idx // n_cols\n",
    "    col_idx = idx % n_cols\n",
    "    axes2[row, col_idx].boxplot(df_clean[col].dropna(), patch_artist=True, boxprops=dict(facecolor='lightgreen', alpha=0.7))\n",
    "    axes2[row, col_idx].set_title(f'{col} - Box Plot', fontweight='bold')\n",
    "    axes2[row, col_idx].set_ylabel(col)\n",
    "    axes2[row, col_idx].grid(True, alpha=0.3)\n",
    "\n",
    "# Hide any unused subplots in boxplots\n",
    "for idx in range(len(numeric_columns), n_rows * n_cols):\n",
    "    row = idx // n_cols\n",
    "    col_idx = idx % n_cols\n",
    "    fig2.delaxes(axes2[row, col_idx])\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8734f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CATEGORICAL VARIABLES PLOTS\n",
    "categorical_columns = ['brand', 'model', 'fuel_type', 'engine', 'transmission', 'ext_col', 'int_col', 'accident', 'clean_title']\n",
    "\n",
    "# Split into 3 groups of 3 columns each (6 charts per group)\n",
    "group1 = categorical_columns[:3]  # First 3 columns: brand, model, fuel_type\n",
    "group2 = categorical_columns[3:6]  # Next 3 columns: engine, transmission, ext_col\n",
    "group3 = categorical_columns[6:]  # Last 3 columns: int_col, accident, clean_title\n",
    "\n",
    "# FIGURE 1: First 3 columns (6 charts)\n",
    "fig1, axes1 = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig1.suptitle('Categorical Variables', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, col in enumerate(group1):\n",
    "    value_counts = df_clean[col].value_counts().head(8)\n",
    "    \n",
    "    # Bar chart\n",
    "    axes1[0, i].bar(range(len(value_counts)), value_counts.values, \n",
    "                   color='lightcoral', alpha=0.7, edgecolor='black')\n",
    "    axes1[0, i].set_title(f'{col}', fontweight='bold')\n",
    "    axes1[0, i].set_xlabel(col)\n",
    "    axes1[0, i].set_ylabel('Count')\n",
    "    axes1[0, i].tick_params(axis='x', rotation=45)\n",
    "    axes1[0, i].grid(True, alpha=0.3)\n",
    "    axes1[0, i].set_xticks(range(len(value_counts)))\n",
    "    axes1[0, i].set_xticklabels(value_counts.index, rotation=45, ha='right')\n",
    "    \n",
    "    # Pie chart\n",
    "    axes1[1, i].pie(value_counts.values, labels=value_counts.index, \n",
    "                   autopct='%1.1f%%', startangle=90)\n",
    "    axes1[1, i].set_title(f'{col}', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# FIGURE 2: Next 3 columns (6 charts)\n",
    "fig2, axes2 = plt.subplots(2, 3, figsize=(18, 12))\n",
    "#fig2.suptitle('Categorical Variables - Group 2 (Bar + Pie Charts)', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, col in enumerate(group2):\n",
    "    value_counts = df_clean[col].value_counts().head(8)\n",
    "    \n",
    "    # Bar chart\n",
    "    axes2[0, i].bar(range(len(value_counts)), value_counts.values, \n",
    "                   color='lightblue', alpha=0.7, edgecolor='black')\n",
    "    axes2[0, i].set_title(f'{col}', fontweight='bold')\n",
    "    axes2[0, i].set_xlabel(col)\n",
    "    axes2[0, i].set_ylabel('Count')\n",
    "    axes2[0, i].tick_params(axis='x', rotation=45)\n",
    "    axes2[0, i].grid(True, alpha=0.3)\n",
    "    axes2[0, i].set_xticks(range(len(value_counts)))\n",
    "    axes2[0, i].set_xticklabels(value_counts.index, rotation=45, ha='right')\n",
    "    \n",
    "    # Pie chart\n",
    "    axes2[1, i].pie(value_counts.values, labels=value_counts.index, \n",
    "                   autopct='%1.1f%%', startangle=90)\n",
    "    axes2[1, i].set_title(f'{col}', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# FIGURE 3: Last 3 columns (6 charts)\n",
    "fig3, axes3 = plt.subplots(2, 3, figsize=(18, 12))\n",
    "#fig3.suptitle('Categorical Variables - Group 3 (Bar + Pie Charts)', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, col in enumerate(group3):\n",
    "    value_counts = df_clean[col].value_counts().head(8)\n",
    "    \n",
    "    # Bar chart\n",
    "    axes3[0, i].bar(range(len(value_counts)), value_counts.values, \n",
    "                   color='lightgreen', alpha=0.7, edgecolor='black')\n",
    "    axes3[0, i].set_title(f'{col}', fontweight='bold')\n",
    "    axes3[0, i].set_xlabel(col)\n",
    "    axes3[0, i].set_ylabel('Count')\n",
    "    axes3[0, i].tick_params(axis='x', rotation=45)\n",
    "    axes3[0, i].grid(True, alpha=0.3)\n",
    "    axes3[0, i].set_xticks(range(len(value_counts)))\n",
    "    axes3[0, i].set_xticklabels(value_counts.index, rotation=45, ha='right')\n",
    "    \n",
    "    # Pie chart\n",
    "    axes3[1, i].pie(value_counts.values, labels=value_counts.index, \n",
    "                   autopct='%1.1f%%', startangle=90)\n",
    "    axes3[1, i].set_title(f'{col}', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cd4a86",
   "metadata": {},
   "source": [
    "## Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456d2b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dython.nominal import associations\n",
    "import seaborn as sns\n",
    "\n",
    "# matriz de correlacion incluyendo variables categoricas\n",
    "def matriz_correlacion_categoricas(df):\n",
    "    associations_df = associations(df_clean, nominal_columns=categorical_columns, plot=False)\n",
    "    corr_matrix = associations_df['corr']\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.title('Correlation Matrix Including Categorical Variables', fontsize=16)\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True, square=True, linewidths=.5)\n",
    "    plt.show()\n",
    "\n",
    "matriz_correlacion_categoricas(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc1f6f9",
   "metadata": {},
   "source": [
    "## Random Forest model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4264d3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "X = df_clean.drop(columns=['price'])\n",
    "y = df_clean['price']\n",
    "\n",
    "cat_cols = X.select_dtypes(include=['object']).columns\n",
    "num_cols = X.select_dtypes(include=['number']).columns\n",
    "\n",
    "#First we need to encode categorical columns so the model can be trained\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols),\n",
    "        ('num', 'passthrough', num_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Reduce overfitting by limiting tree depth, minimum samples per leaf and minimum samples to split a node\n",
    "# parameters were optimized with search_grid, but took 23 minutes to execute, taht is why has not been included here\n",
    "model = Pipeline(steps=[\n",
    "    ('prep', preprocess),\n",
    "    ('reg', RandomForestRegressor(random_state=1, n_estimators=100, max_depth=7, min_samples_leaf=10, min_samples_split=2))\n",
    "])\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n",
    "model.fit(train_X, train_y)\n",
    "\n",
    "# Predict with all validation observations\n",
    "val_predictions = model.predict(val_X)\n",
    "train_predictions = model.predict(train_X)\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Display predictions and actual prices in parallel columns for validation and training sets using HTML tables (no tabulate dependency)\n",
    "def make_html_table(df, title):\n",
    "    html = f'<b>{title}</b><br><table><tr>'\n",
    "    for col in df.columns:\n",
    "        html += f'<th>{col}</th>'\n",
    "    html += '</tr>'\n",
    "    for _, row in df.iterrows():\n",
    "        html += '<tr>' + ''.join(f'<td>{v}</td>' for v in row) + '</tr>'\n",
    "    html += '</table>'\n",
    "    return html\n",
    "\n",
    "val_df = pd.DataFrame({\n",
    "    'Validation Prediction': val_predictions[:5],\n",
    "    'Validation Actual': list(val_y[:5])\n",
    "})\n",
    "train_df = pd.DataFrame({\n",
    "    'Train Prediction': train_predictions[:5],\n",
    "    'Train Actual': list(train_y[:5])\n",
    "})\n",
    "\n",
    "output = []\n",
    "output.append(\"<br>\" + \"=\"*60)\n",
    "output.append(\"RANDOM FOREST REGRESSOR RESULTS\")\n",
    "output.append(\"=\"*60)\n",
    "output.append(make_html_table(val_df, \"Validation Set (Top 5)\"))\n",
    "output.append(make_html_table(train_df, \"Training Set (Top 5)\"))\n",
    "output.append(\"-\"*60)\n",
    "val_mae = mean_absolute_error(val_y, val_predictions)\n",
    "train_mae = mean_absolute_error(train_y, train_predictions)\n",
    "val_r2 = r2_score(val_y, val_predictions)\n",
    "train_r2 = r2_score(train_y, train_predictions)\n",
    "output.append(f\"Validation MAE: {val_mae:.2f}\")\n",
    "output.append(\"<br>\")\n",
    "output.append(f\"Training MAE: {train_mae:.2f}\")\n",
    "output.append(\"<br>\")\n",
    "output.append(f\"Validation R2: {val_r2:.3f}\")\n",
    "output.append(\"<br>\")\n",
    "output.append(f\"Training R2: {train_r2:.3f}\")\n",
    "display(Markdown('  \\n'.join(output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c8ad6a",
   "metadata": {},
   "source": [
    "## Train model with whole dataset and create csv file with predictions to feed it into the frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27bcdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model on the entire df_clean and generate predictions for the frontend\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Prepare features and target\n",
    "X_full = df_clean.drop(columns=['price'])\n",
    "y_full = df_clean['price']\n",
    "\n",
    "cat_cols = X_full.select_dtypes(include=['object']).columns\n",
    "num_cols = X_full.select_dtypes(include=['number']).columns\n",
    "\n",
    "preprocess = ColumnTransformer([\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols),\n",
    "    ('num', 'passthrough', num_cols)\n",
    "], remainder='drop')\n",
    "\n",
    "model = Pipeline([\n",
    "    ('prep', preprocess),\n",
    "    ('reg', RandomForestRegressor(random_state=1, n_estimators=100, max_depth=7, min_samples_leaf=10, min_samples_split=2))\n",
    "])\n",
    "\n",
    "model.fit(X_full, y_full)\n",
    "\n",
    "# Predict prices for the whole dataset\n",
    "predicted_prices = model.predict(X_full)\n",
    "\n",
    "# Create a new DataFrame with all features and predicted price\n",
    "df_pred = X_full.copy()\n",
    "df_pred['predicted_price'] = predicted_prices\n",
    "\n",
    "# Save to CSV for frontend use\n",
    "df_pred.to_csv('../datasets/predicted_prices_for_frontend.csv', index=False)\n",
    "df_pred.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9057646,
     "sourceId": 76728,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9.775794,
   "end_time": "2025-09-19T10:17:01.011402",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-19T10:16:51.235608",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
